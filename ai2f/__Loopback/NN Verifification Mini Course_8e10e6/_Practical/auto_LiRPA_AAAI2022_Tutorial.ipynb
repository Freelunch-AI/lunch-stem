{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2151a9b1ec654666ab287e62b62e6648": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_852a086d4f2b4405a4eefc89c4b7a36e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9dc954411ea3487982c8d4c7ecb72af6",
              "IPY_MODEL_a2d065faf6824da09824a78a26807ada",
              "IPY_MODEL_a1cf43ad069e4344a27a25bafe06a69b"
            ]
          }
        },
        "852a086d4f2b4405a4eefc89c4b7a36e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9dc954411ea3487982c8d4c7ecb72af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0dd6014f73e848298123ba68c9cd49c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_03cd3dc8f3c44bd3aa7edfdda4a7f507"
          }
        },
        "a2d065faf6824da09824a78a26807ada": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bfdb8cc9b94a4c0ba3d3b95f4a457c99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 170498071,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 170498071,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_950791a6e7ae4f8cbb4eb21c7484bca1"
          }
        },
        "a1cf43ad069e4344a27a25bafe06a69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1150c5b94f6c42abb28b25fda976c553",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170499072/? [00:03&lt;00:00, 59393159.95it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d725036ef1742dbb4a4a25a73ddd92e"
          }
        },
        "0dd6014f73e848298123ba68c9cd49c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "03cd3dc8f3c44bd3aa7edfdda4a7f507": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bfdb8cc9b94a4c0ba3d3b95f4a457c99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "950791a6e7ae4f8cbb4eb21c7484bca1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1150c5b94f6c42abb28b25fda976c553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d725036ef1742dbb4a4a25a73ddd92e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu6MUQmRUR_i"
      },
      "source": [
        "# `auto_LiRPA` Quick start tutorial (AAAI 2022)\n",
        "\n",
        "auto_LiRPA is a library for automatically deriving and computing bounds with linear relaxation based perturbation analysis (LiRPA) (e.g. CROWN and DeepPoly) for neural networks. LiRPA algorithms can provide guaranteed upper and lower bounds for a neural network function with perturbed inputs. These bounds are represented as linear functions with respect to the variable under perturbation. LiRPA has become an important tool in robustness verification and certified adversarial defense, and can become an useful tool for many other tasks as well.\n",
        "\n",
        "Our algorithm generalizes existing LiRPA algorithms for feed-forward neural networks to a graph algorithm on general computational graphs. We can compute LiRPA bounds on a computational graph defined by PyTorch, without any manual derivation. Our implementation is also automatically differentiable, allowing optimizing network parameters to shape the bounds into certain specifications (e.g., certified defense)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjYhHLlxHI3K"
      },
      "source": [
        "## Installation & Imports\n",
        "We first install the auto_LiRPA library using pip. Note that our library is tested on Pytorch 1.8.2 LTS, and other versions might be incompatible."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKSD0dEh2u4"
      },
      "source": [
        "# Uninstall existing Pytorch on Colab, which might be incompatible or buggy.\n",
        "!pip uninstall --yes torch torchvision torchaudio torchtext\n",
        "# Install Pytorch 1.8.2 LTS (Long Term Support) and auto_LiRPA. It might take a few minutes depending on network speed.\n",
        "!pip install torch==1.8.2+cu102 torchvision==0.9.2+cu102 torchaudio==0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
        "!pip install git+https://github.com/KaidiXu/auto_LiRPA.git\n",
        "# Clear installation output to avoid clutter.\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQJWL04XK0Fx"
      },
      "source": [
        "Common Pytorch imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-f-iqIZJ0Ph"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGFQGxgwK9zw"
      },
      "source": [
        "Imports for using auto_LiRPA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOFxF3WcKyfA"
      },
      "source": [
        "from auto_LiRPA import BoundedModule, BoundedTensor\n",
        "from auto_LiRPA.perturbations import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg9bR06eHf4Y"
      },
      "source": [
        "## Define the Computation (Neural Network)\n",
        "To begin with, we define a **18-layer ResNet** using Pytorch. The network is defined as a standard nn.module object in Pytorch, and consists of **convolutional**, **pooling** and **batch normalization** layers. We will use our auto_LiRPA library to compute bounds for this network automatically, without manual derivations of the bounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtKhZTQwVPpx"
      },
      "source": [
        "'''ResNet in PyTorch.\n",
        "For Pre-activation ResNet, see 'preact_resnet.py'.\n",
        "Reference:\n",
        "[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n",
        "    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n",
        "'''\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, in_planes=64):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = in_planes\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
        "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, in_planes * 2, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, in_planes * 4, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, in_planes * 8, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(in_planes * 8 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18(in_planes=2):\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2], in_planes=in_planes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zPT4REmOzqL"
      },
      "source": [
        "Now we create the model, and load some pretrained parameters for demonstration. Note that this pretrained model was naturally trained so only verifiable under small perturbations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3IrpkFtjOwg",
        "outputId": "5bbaffb8-6e99-4f6e-cb19-6f9b78658fe8"
      },
      "source": [
        "model = ResNet18()\n",
        "# Download the model\n",
        "!wget -O resnet18_demo.pth http://download.huan-zhang.com/models/auto_lirpa/resnet18_natural.pth\n",
        "# Load pretrained weights. This pretrained model is for illustration only; it\n",
        "# does not represent state-of-the-art classification performance.\n",
        "checkpoint = torch.load(\"resnet18_demo.pth\")\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model.eval()\n",
        "model = model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-02-23 17:51:07--  http://download.huan-zhang.com/models/auto_lirpa/resnet18_natural.pth\n",
            "Resolving download.huan-zhang.com (download.huan-zhang.com)... 104.21.96.11, 172.67.171.242, 2606:4700:3030::6815:600b, ...\n",
            "Connecting to download.huan-zhang.com (download.huan-zhang.com)|104.21.96.11|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 208801 (204K)\n",
            "Saving to: ‘resnet18_demo.pth’\n",
            "\n",
            "resnet18_demo.pth   100%[===================>] 203.91K  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-02-23 17:51:08 (1.61 MB/s) - ‘resnet18_demo.pth’ saved [208801/208801]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ1XVLnQPDCE"
      },
      "source": [
        "## Load dataset\n",
        "\n",
        "We simply use the standard CIFAR-10 dataset. We load a random image from the dataset for demonstrating the usage of our framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVWoWCj0nVjR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "2151a9b1ec654666ab287e62b62e6648",
            "852a086d4f2b4405a4eefc89c4b7a36e",
            "9dc954411ea3487982c8d4c7ecb72af6",
            "a2d065faf6824da09824a78a26807ada",
            "a1cf43ad069e4344a27a25bafe06a69b",
            "0dd6014f73e848298123ba68c9cd49c2",
            "03cd3dc8f3c44bd3aa7edfdda4a7f507",
            "bfdb8cc9b94a4c0ba3d3b95f4a457c99",
            "950791a6e7ae4f8cbb4eb21c7484bca1",
            "1150c5b94f6c42abb28b25fda976c553",
            "4d725036ef1742dbb4a4a25a73ddd92e"
          ]
        },
        "outputId": "f269bacc-1f5b-43c2-b2e8-8d7db3d977b7"
      },
      "source": [
        "test_data = datasets.CIFAR10(\n",
        "    \"./data\", train=False, download=True, \n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[0.2023, 0.1994, 0.2010])]))\n",
        "# Choose one image from the dataset.\n",
        "idx = 123\n",
        "image = test_data[idx][0].view(1,3,32,32).cuda()\n",
        "label = data = test_data[idx][1]\n",
        "print('Ground-truth label:', label)\n",
        "print('Model prediction:', model(image))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2151a9b1ec654666ab287e62b62e6648",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Ground-truth label: 2\n",
            "Model prediction: tensor([[-2.1683, -6.2335,  5.0832, -2.8249, -3.9203, -2.3359, -2.0199, -3.7470,\n",
            "         -7.4981, -5.9163]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXmWvpTHFN-0"
      },
      "source": [
        "## Use `auto_LiRPA` to obtain provable lower and outer bounds under perturbation\n",
        "\n",
        "There are three essential steps to use `auto_LiRPA`:\n",
        "\n",
        "1.   Wrap a predefined computation in a `nn.Module` object with `auto_LiRPA.BoundedModule`;\n",
        "2.   Define perturbation as a `BoundedTensor` (or `BoundedParameter` if you are perturbing model weights);\n",
        "3.   Use the `compute_bounds()` method to obtain lower and upper bounds of the computational graph defined in `nn.Module`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Uck9k6f6Yz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823a79ef-c1b1-499b-cbf4-2534436ae343"
      },
      "source": [
        "# Step 1: wrap model with BoundedModule. \n",
        "bounded_model = BoundedModule(model=model, global_input=torch.zeros_like(image))\n",
        "bounded_model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:2113: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  if size_prods == 1:\n",
            "/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:680: UserWarning: ONNX export mode is set to inference mode, but operator batch_norm is set to training  mode. The model will be exported in inference, as specified by the export mode.\n",
            "  training_mode + \", as specified by the export mode.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWDY87o-gLi-",
        "outputId": "b468d882-1a87-4868-8da6-d4e8fc92dbef"
      },
      "source": [
        "# Step 2: define perturbation. Here we use a Linf perturbation on input image.\n",
        "eps = 0.003\n",
        "norm = np.inf\n",
        "ptb = PerturbationLpNorm(norm=norm, eps=eps)\n",
        "# Input tensor is wrapped in a BoundedTensor object.\n",
        "bounded_image = BoundedTensor(x=image, ptb=ptb)\n",
        "# We can use BoundedTensor to get model prediction as usual. Regular forward/backward propagation is unaffected.\n",
        "print('Model prediction:', bounded_model(bounded_image))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction: tensor([[-2.1683, -6.2335,  5.0832, -2.8249, -3.9203, -2.3359, -2.0199, -3.7470,\n",
            "         -7.4981, -5.9163]], device='cuda:0', grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7lVE11xuXVy"
      },
      "source": [
        "As you can see above, the `BoundedModule` object wrapped by `auto_LiRPA` can be used the same way as a regular Pytorch model, with a `BoundedTensor` as its input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY2CZiC7g5oX",
        "outputId": "2f3cc698-40ea-4134-c6d1-4f1ca1bf5a32"
      },
      "source": [
        "# Step 3: compute bounds using the compute_bounds() method.\n",
        "print('Bounding method: backward (CROWN, DeepPoly)')\n",
        "with torch.no_grad():  # If gradients of the bounds are not needed, we can use no_grad to save memory.\n",
        "  lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='CROWN')\n",
        "\n",
        "# Auxillary function to print bounds.\n",
        "def print_bounds(lb, ub):\n",
        "    lb = lb.detach().cpu().numpy()\n",
        "    ub = ub.detach().cpu().numpy()\n",
        "    for j in range(10):\n",
        "        print(\"f_{j}(x_0): {l:8.3f} <= f_{j}(x_0+delta) <= {u:8.3f}\".format(\n",
        "            j=j, l=lb[0][j], u=ub[0][j], r=ub[0][j] - lb[0][j]))\n",
        "\n",
        "print_bounds(lb, ub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bounding method: backward (CROWN, DeepPoly)\n",
            "f_0(x_0):   -5.638 <= f_0(x_0+delta) <=    0.521\n",
            "f_1(x_0):  -10.532 <= f_1(x_0+delta) <=   -2.419\n",
            "f_2(x_0):    1.883 <= f_2(x_0+delta) <=    7.537\n",
            "f_3(x_0):   -5.327 <= f_3(x_0+delta) <=   -0.827\n",
            "f_4(x_0):   -7.217 <= f_4(x_0+delta) <=   -1.037\n",
            "f_5(x_0):   -5.238 <= f_5(x_0+delta) <=   -0.151\n",
            "f_6(x_0):   -5.686 <= f_6(x_0+delta) <=    0.118\n",
            "f_7(x_0):   -7.934 <= f_7(x_0+delta) <=   -0.303\n",
            "f_8(x_0):  -12.044 <= f_8(x_0+delta) <=   -3.793\n",
            "f_9(x_0):   -9.329 <= f_9(x_0+delta) <=   -3.074\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZG3TVOjzuplp"
      },
      "source": [
        "The backward mode perturbation analysis (an extension of [CROWN](https://https://arxiv.org/pdf/1811.00866.pdf)) provides relatively tight bounds. In this example above, the ground-truth label is 2. You can see that the model logit output for label 2 is bounded between 1.883 and 7.537, and we can guarantee that its the top-1 label under perturbation.\n",
        "\n",
        "Next, we will compute the bounds using interval bound propagation (IBP), a previous approach that can also operate on general computational graphs. However, it produces much looser and vacuous bounds:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukIFpofCitDI",
        "outputId": "1eae77ac-2eaa-42d4-c99e-0e6a59d27c4a"
      },
      "source": [
        "# Our library also supports the interval bound propagation (IBP) based bounds, \n",
        "# but it produces much looser bounds.\n",
        "print('Bounding method: IBP')\n",
        "with torch.no_grad():\n",
        "  lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='IBP')\n",
        "\n",
        "print_bounds(lb, ub)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bounding method: IBP\n",
            "f_0(x_0): -23917152.000 <= f_0(x_0+delta) <= 14821585.000\n",
            "f_1(x_0): -25477740.000 <= f_1(x_0+delta) <= 16557554.000\n",
            "f_2(x_0): -18018624.000 <= f_2(x_0+delta) <= 13646834.000\n",
            "f_3(x_0): -17182962.000 <= f_3(x_0+delta) <= 9431992.000\n",
            "f_4(x_0): -22261390.000 <= f_4(x_0+delta) <= 12147498.000\n",
            "f_5(x_0): -21668388.000 <= f_5(x_0+delta) <= 12951016.000\n",
            "f_6(x_0): -24474524.000 <= f_6(x_0+delta) <= 11607180.000\n",
            "f_7(x_0): -28624064.000 <= f_7(x_0+delta) <= 17297988.000\n",
            "f_8(x_0): -29272032.000 <= f_8(x_0+delta) <= 17333456.000\n",
            "f_9(x_0): -24436304.000 <= f_9(x_0+delta) <= 12459551.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8204Z-uEtiL"
      },
      "source": [
        "## Differentiability of our bounds\n",
        "\n",
        "The bounds obtained by our `compute_bounds()` method are themselves differentiable w.r.t. input image or model parameters. We can obtain the gradients easily just as we usually do in Pytorch.  The gradients can be used for certified defense training. See our [training examples](https://github.com/KaidiXu/auto_LiRPA#basic-certified-training).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEcnAkUxjMHC",
        "outputId": "ec85d637-26ed-48fe-8e8d-5f6be9dbc4ba"
      },
      "source": [
        "bounded_model.zero_grad()\n",
        "lb, ub = bounded_model.compute_bounds(x=(bounded_image,), method='CROWN')\n",
        "# Create a dummy scalar function for demonstrating the differentiability.\n",
        "loss = lb.sum()\n",
        "loss.backward()\n",
        "# This is the gradients of the loss w.r.t. first convolutional layer's weights:\n",
        "print('grad norm:', list(model.modules())[1].weight.grad.norm(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad norm: tensor(97.1450, device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oSAAEAkUv9a"
      },
      "source": [
        "## More examples\n",
        "\n",
        "We provide many examples of `auto_LiRPA` in our repository. You can find more details of these examples [here](https://github.com/KaidiXu/auto_LiRPA#more-working-examples). Notably, we provided the following examples for `auto_LiRPA`:\n",
        "\n",
        "1. Certified defense on CIFAR-10, **TinyImageNet** and **ImageNet** (64*64) using large scale computer vision models such as DenseNet, ResNeXt and WideResNet.\n",
        "2. Examples on using **loss fusion**, an efficient technique that scales linear relaxation based certified defense to large datasets, making certified defense training up to 1000 times faster compared to the previous approach.\n",
        "3. Examples on training verifiably robust **LSTM** and **Transformer** models on natural language processing (**NLP**) tasks.\n",
        "4. Examples on bounding network output given **model weight perturbations**. Existing frameworks can only handle perturbations on model inputs, not on model parameters (weights). This allows us to perform robustness verification or certified adversarial defense against weight perturbations. We can also train the bounds on model weights to obtain models with flat optimization landscapes. "
      ]
    }
  ]
}