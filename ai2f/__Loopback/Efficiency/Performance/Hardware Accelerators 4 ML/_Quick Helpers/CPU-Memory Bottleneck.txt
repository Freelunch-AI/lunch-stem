Architectures specialized for AI applications seek to reduce the CPU:memory bottleneck. For example, the AI accelerator chip that IBM described at the 2018 VLSI Circuits Symposium combines processing elements designed to speed matrix multiplication with a “scratchpad memory” hierarchy used to reduce returns to off-chip memory (Figure 7). Emerging advanced AI chips similarly utilize various methods to merge logic and memory in microarchitectures needed to speed the kinds of operations underlying AI applications.