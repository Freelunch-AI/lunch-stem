{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Layer\n",
    "In this notebook, we will look into the forward and the backward the the ```nn.SpatialConvolution``` layer. We will also see how to compute the gradient of the loss with respect to the weights $\\frac{\\partial L}{\\partial W}$ for this layer. I leave gradient of the loss with respect to the input $\\frac{\\partial L}{\\partial I}$ as an excercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input\n",
    "Similar to the example we used in the class, let us have the input $n$ of size $1 \\times 4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  0.9741  0.6880  0.2478  0.5842\n",
       "[torch.DoubleTensor of size 1x1x4]\n",
       "\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "require 'nn';\n",
    "n = torch.rand(4):reshape(1,1,4)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output using Convolution Block\n",
    "Also similar to the example we used in the class, let us create a convolution block with a weights $w$ of size $1 \\times 3$ and obtain the output $m = Convolution(n, w)$ of size $1\\times2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  0.1151 -0.0816\n",
       "[torch.DoubleTensor of size 1x1x2]\n",
       "\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.SpatialConvolutionMM(1,1,3,1)\n",
    "conv.bias:fill(0)\n",
    "m = conv:forward(n)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing backward and calculating the gradinent of the loss with respect to the weights\n",
    "Let us set the gradient of the loss with respect to the input of next layer (flowing in through the next layer) $\\frac{\\partial L}{\\partial I^{l+1}}$ to be random numbers. After the ```backward()``` call, let us print the $\\frac{\\partial L}{\\partial W}$ as calcuated by torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.3516  0.1675  0.2284\n",
       "[torch.DoubleTensor of size 1x3]\n",
       "\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nextgrad=torch.rand(2):reshape(1,1,2)\n",
    "conv:backward(n, nextgrad)\n",
    "print(conv.gradWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expression for calcuating the gradinent of the loss with respect to the weights\n",
    "Again, as we learnt in class, the $\\frac{\\partial L}{\\partial W} = Convolution(I, \\frac{\\partial L}{\\partial I^{l+1}})$. We varify that this is indeed exactly equal to $\\frac{\\partial L}{\\partial W}$ computed by torch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,.,.) = \n",
       "  0.3516  0.1675  0.2284\n",
       "[torch.DoubleTensor of size 1x1x3]\n",
       "\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convback = nn.SpatialConvolutionMM(1,1,2,1)\n",
    "convback.bias:fill(0)\n",
    "convback.weight:copy(nextgrad:reshape(1,2))\n",
    "gradWeight = convback:forward(n)\n",
    "print(gradWeight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I leave the calculation of $\\frac{\\partial L}{\\partial I}$ as an exercise. Remember: $\\frac{\\partial L}{\\partial I} = Convolution(W^{padded}, \\frac{\\partial L}{\\partial I^{l+1}}^{Flip})$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
