{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"- University: University of SÃ£o Paulo (USP) \n\n- Class: PMR3508 (2021) - Fundamentals of Machine Learning\n\n- Kaggle Competition: Adult","metadata":{}},{"cell_type":"markdown","source":"# Table of contents\n1. [Setup and imports](#Setup-and-imports)\n    1. [Libraries](##Libraries)\n    2. [Setup](##Setup)\n2. [EDA (Exploratory Data Analysis)](#EDA(Exploratory-Data-Analysis))\n    1. [Glance at data](##Glance-at-data)\n        1. [Train dataset](###Train-dataset)\n        2. [Test dataset](###Test-dataset)\n    2. [Summary statistics](##Summary-statistics)\n    3. [Target histogram](##Target-histogram)\n    4. [Non zero counts](##Non-zero-counts)\n    5. [Empirical distribution of features](##Empirical-distribution-of-features)\n        1. [Train dataset](###Train-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        2. [Test dataset](###Test-dataset)\n            1. [Histograms of numerical features](####Histograms-of-numerical-features)\n            2. [Bar plots for categorical features](####Barplots-for-categorical-features)\n        3. [Plots of target vs features](###Plots-of-target-vs-features)\n            1. [Numerical features](####Numerical-features)\n            2. [Categorical features](####Categorical-features)\n        4. [Pairwise plots](###Pairwise-plots)\n            1. [Scatter plot](####Numerical-vs-numerical)\n            2. [Correlation heatmap](####Correlation-heatmap)\n            3. [Categorical heatmap](####Categorical-heatmap)\n3. [Data engineering](#Data-engineering)\n    1. [Divide dataset into numerical and categorical subdatasets](##Divide-dataset-into-numerical-and-categorical-subdatasets)\n    1. [Normalize features](##Normalize-features)\n    2. [Treat categorical features](##Treat-categorical-features)\n    3. [Joining numerical and categorical dfs back](##Joining-numerical-and-categorical-dfs-back)\n    4. [Treat missing values](##Treat-missing-values)\n    5. [Treat outliers](##Treat-outliers)\n    6. [Feature tranformations](##Feature-tranformations)\n    7. [Mirror on test dataset](##Mirror-on-testdataset)\n4. [Feature Engineering](#Featur-engineering)\n    1. [Importance sampling](##Importance-sampling)\n    2. [Select features](##Select-features)\n    3. [Create new features](##Create-new-features)\n5. [Experiments](#Experiments)\n    1. [Base dataset](##Base-dataset)\n    2. [Baseline (KNN)](##Baseline-(KNN))\n    3. [4 Classifiers](##4-Classifiers)\n        1. [RF](###RF)\n        2. [XGBoost](###XGBoost)\n        3. [SVM](###SVM)\n        4. [NN](###NN)\n    4. [Engineered datasets](##Engineered-datasets)\n6. [Final model](#Final-model)\n7. [Submission](#Submission)","metadata":{}},{"cell_type":"markdown","source":"# Setup and imports\n### Setup environment and import libraries.","metadata":{}},{"cell_type":"markdown","source":"## Libraries","metadata":{}},{"cell_type":"code","source":"import sys\nimport copy\n\n# Just because I have a conflicting Python 3.6 installation at root\n# Comment this when uploading to kaggle\n# try:\n#     sys.path.remove('C:/Python36/Lib/site-packages')\n#     sys.path.remove('C:/Python36/Lib')\n# except:\n#     print(\"py36 not influencing\")\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn as sk\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:24.118097Z","iopub.execute_input":"2021-11-30T13:12:24.118426Z","iopub.status.idle":"2021-11-30T13:12:25.561139Z","shell.execute_reply.started":"2021-11-30T13:12:24.118338Z","shell.execute_reply":"2021-11-30T13:12:25.560394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setup","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nsns.set()\n\n# Loading data\n# adultTrain = pd.read_csv(\n#     \"C:/Users/bruno/Desktop/kaggle-adult-comp-knn/data/train_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# # For uploading to kaggle\nadultTrain = pd.read_csv(\n    \"/kaggle/input/adult-pmr3508/train_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\n# adultTest = pd.read_csv(\n#     \"C:/Users/bruno/Desktop/kaggle-adult-comp-knn/data/test_data.csv\",\n#     sep=r'\\s*,\\s*',\n#     engine='python',\n#     na_values=\"?\",\n# )\n\n# # For uploading to kaggle\nadultTest = pd.read_csv(\n    \"/kaggle/input/adult-pmr3508/test_data.csv\",\n    sep=r'\\s*,\\s*',\n    engine='python',\n    na_values=\"?\",\n)\n\nmodifyNames = {\n    \"fnlwgt\": \"weight\",\n    \"education.num\": \"educationNum\", \n    \"marital.status\": \"maritalStatus\",\n    \"capital.gain\": \"capitalGain\", \n    \"capital.loss\": \"capitalLoss\",\n    \"hours.per.week\": \"hoursPerWeek\", \n    \"native.country\": \"country\",\n    \"income\": \"target\"\n}\n\n# Changing columns names\nadultTrain.rename(columns=modifyNames, inplace=True)\nadultTest.rename(columns=modifyNames, inplace=True)\n\n# Casting appropriate datatypes\ndtypes = {\n    \"age\": int,\n    \"workclass\": str,\n    \"weight\": int,             \n    \"education\": str,\n    \"educationNum\": int,\n    \"maritalStatus\": str,\n    \"occupation\": str,\n    \"relationship\": str,\n    \"race\": str,\n    \"sex\": str,\n    \"capitalGain\": int,\n    \"capitalLoss\": int,\n    \"hoursPerWeek\": int,\n    \"country\": str,\n    \"target\": str\n}\n\nadultTrain.astype(dtypes, copy=False)\nadultTest.astype(dtypes.pop(\"target\"), copy=False)\n\n# Id is not relevant, so it is dropped\nadultTrain.pop(\"Id\")\nidTest = adultTest.pop(\"Id\")\n\n# weight is not important for testing\nweightTrain = adultTrain[\"weight\"]\nadultTest.pop(\"weight\")\n\nprint(\"\\n\\n#### TRAIN DATASET ####\")\n# (32560, 16)\nprint('\\nshape: ', adultTrain.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTrain.dtypes)\n# max of 4000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTrain.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTrain.duplicated().sum()) \n\nprint(\"\\n\\n#### TEST DATASET ####\")\n# (16280, 15)\nprint('\\nshape: ', adultTest.shape)\n# all as objects, need to change some datatypes\nprint('\\ndata types:\\n', adultTest.dtypes)\n# max of aprox 2000 datapoints with some nan entry -> treat them\nprint('\\nNumber of null entries:\\n', adultTest.isnull().sum())\n# No duplicated data points\nprint('\\nDuplicated data points:\\n', adultTest.duplicated().sum()) ","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:25.564758Z","iopub.execute_input":"2021-11-30T13:12:25.565005Z","iopub.status.idle":"2021-11-30T13:12:26.512725Z","shell.execute_reply.started":"2021-11-30T13:12:25.564972Z","shell.execute_reply":"2021-11-30T13:12:26.511604Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# EDA (Exploratory Data Analysis)\n### Get to know data and draw insights on the problem of classifying income as > 50K.","metadata":{}},{"cell_type":"markdown","source":"## Glance at data","metadata":{}},{"cell_type":"markdown","source":"### Train dataset","metadata":{}},{"cell_type":"code","source":"# education can be dropped, since educationNum is givving all the information we want\n# there is notinh specific about a certain degree that will affect the target\nadultTrain.head(20)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.513976Z","iopub.execute_input":"2021-11-30T13:12:26.5142Z","iopub.status.idle":"2021-11-30T13:12:26.540083Z","shell.execute_reply.started":"2021-11-30T13:12:26.514172Z","shell.execute_reply":"2021-11-30T13:12:26.539116Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test dataset","metadata":{}},{"cell_type":"code","source":"adultTest.head(10)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.54199Z","iopub.execute_input":"2021-11-30T13:12:26.542211Z","iopub.status.idle":"2021-11-30T13:12:26.557866Z","shell.execute_reply.started":"2021-11-30T13:12:26.542184Z","shell.execute_reply":"2021-11-30T13:12:26.557158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Summary statistics","metadata":{}},{"cell_type":"code","source":"adultTrain.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.55909Z","iopub.execute_input":"2021-11-30T13:12:26.559318Z","iopub.status.idle":"2021-11-30T13:12:26.605584Z","shell.execute_reply.started":"2021-11-30T13:12:26.559269Z","shell.execute_reply":"2021-11-30T13:12:26.604732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##  Target histogram","metadata":{}},{"cell_type":"code","source":"# aprox 25 000 datapoints <= 50K and 7 500 < 50K -> relatively imbalanced dataset\n# most simple baseline is prediciting always <= 50K -> gives 0.76% accuracy\ncounts = adultTrain[\"target\"].value_counts().values\nimbalanceRatio = counts[0]/counts[1]\nprint(imbalanceRatio)\nadultTrain[\"target\"].value_counts().plot(kind=\"bar\")\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.606851Z","iopub.execute_input":"2021-11-30T13:12:26.607138Z","iopub.status.idle":"2021-11-30T13:12:26.810097Z","shell.execute_reply.started":"2021-11-30T13:12:26.607105Z","shell.execute_reply":"2021-11-30T13:12:26.809267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Non zero counts","metadata":{}},{"cell_type":"code","source":"# capitalGain and capitalLoss have very few examples\n# ideas\n    # 1. exclude these festures\n    # 2. cluster them in two bins -> will become boolean variables\nprint(adultTrain.astype(bool).sum(axis=0))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.811367Z","iopub.execute_input":"2021-11-30T13:12:26.811573Z","iopub.status.idle":"2021-11-30T13:12:26.82453Z","shell.execute_reply.started":"2021-11-30T13:12:26.811547Z","shell.execute_reply":"2021-11-30T13:12:26.823379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plot empirical distribution of each feature","metadata":{}},{"cell_type":"markdown","source":"### Train dataset","metadata":{}},{"cell_type":"markdown","source":"#### Histograms of numerical features","metadata":{}},{"cell_type":"code","source":"# hoursPerWeek could be dividid in three bins:  <30, 30-50, >50\n# educationNUm could be dividid in four bins: <8, 8-10, 10-12, >13\n# capitalGains and capitalLoss needs to actuallt only form one feature \n# that is capitalLiquid = capitalGains - capitalLoss. \n# The effect of this feature will be almost as of a imbalanced binary variable since almost all values are zero\n# and the other are in a small range\nadultTrain.hist(bins=30, figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:26.826017Z","iopub.execute_input":"2021-11-30T13:12:26.826363Z","iopub.status.idle":"2021-11-30T13:12:28.543129Z","shell.execute_reply.started":"2021-11-30T13:12:26.82633Z","shell.execute_reply":"2021-11-30T13:12:28.542103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bar plots for categorical features","metadata":{}},{"cell_type":"code","source":"# Private is way bigger than the rest (therefore the rest of the classes have little data)\n# Without pay and never work have very few examples (14) but these examples guarantee we know the target\n# Ideas:\n    # 1. Cluster into 3 bins: private, {without pay + ever worked},  and rest -> \n    # but need to see if private and rest have distinct relatinships with target\nprint('\"Without-pay\" or \"Never-worked\" datapoints: ', adultTrain[adultTrain[\"workclass\"] == (\"Without-pay\" or \"Never-worked\")].shape[0])\nadultTrain[\"workclass\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:28.54466Z","iopub.execute_input":"2021-11-30T13:12:28.544989Z","iopub.status.idle":"2021-11-30T13:12:28.81695Z","shell.execute_reply.started":"2021-11-30T13:12:28.544948Z","shell.execute_reply":"2021-11-30T13:12:28.815853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This feature will be excluded, educationNum already gives us the info we need. There is nothing specific to a \n# certain category that would be relevant for predicting the target\nadultTrain[\"education\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:28.820337Z","iopub.execute_input":"2021-11-30T13:12:28.820594Z","iopub.status.idle":"2021-11-30T13:12:29.167022Z","shell.execute_reply.started":"2021-11-30T13:12:28.820565Z","shell.execute_reply":"2021-11-30T13:12:29.166109Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# A priori a would think only having a present spouse or not is important\n# So this could be cluster into two groups: present spouse and not present spouse\nadultTrain[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:29.168116Z","iopub.execute_input":"2021-11-30T13:12:29.168405Z","iopub.status.idle":"2021-11-30T13:12:29.462681Z","shell.execute_reply.started":"2021-11-30T13:12:29.168363Z","shell.execute_reply":"2021-11-30T13:12:29.461721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Each of the categories seem to be very important\nadultTrain[\"occupation\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:29.46417Z","iopub.execute_input":"2021-11-30T13:12:29.464776Z","iopub.status.idle":"2021-11-30T13:12:29.763105Z","shell.execute_reply.started":"2021-11-30T13:12:29.46473Z","shell.execute_reply":"2021-11-30T13:12:29.762205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This feature seem a little weird, it doest provide mmuch new info, \n# and the categories dont seem to be mutually exclusive\n# Idea: exclude this feature\nadultTrain[\"relationship\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:29.764672Z","iopub.execute_input":"2021-11-30T13:12:29.764909Z","iopub.status.idle":"2021-11-30T13:12:30.0137Z","shell.execute_reply.started":"2021-11-30T13:12:29.764883Z","shell.execute_reply":"2021-11-30T13:12:30.012794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# this could be divided into two bins: white and black \n# because the rest doesnt have data and my guess they would be very similar to white\nadultTrain[\"race\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:30.015011Z","iopub.execute_input":"2021-11-30T13:12:30.015533Z","iopub.status.idle":"2021-11-30T13:12:30.248108Z","shell.execute_reply.started":"2021-11-30T13:12:30.015497Z","shell.execute_reply":"2021-11-30T13:12:30.247108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a priori seems to be important\nadultTrain[\"sex\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:30.249373Z","iopub.execute_input":"2021-11-30T13:12:30.249628Z","iopub.status.idle":"2021-11-30T13:12:30.454913Z","shell.execute_reply.started":"2021-11-30T13:12:30.249588Z","shell.execute_reply":"2021-11-30T13:12:30.453903Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Surely maintaning all these low data categories will fit statistical noise and ruin the accuracy\n# Ideas: \n    # 1. divide in two bins: developed and not developed ccontries\n    # 2. divide in two bins: USA and rest\nadultTrain[\"country\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:30.456314Z","iopub.execute_input":"2021-11-30T13:12:30.456555Z","iopub.status.idle":"2021-11-30T13:12:31.065088Z","shell.execute_reply.started":"2021-11-30T13:12:30.456526Z","shell.execute_reply":"2021-11-30T13:12:31.064219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Test dataset","metadata":{}},{"cell_type":"markdown","source":"#### Histograms of numerical features","metadata":{}},{"cell_type":"code","source":"# no surprises here\nadultTest.hist(bins=30, figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:31.066279Z","iopub.execute_input":"2021-11-30T13:12:31.067049Z","iopub.status.idle":"2021-11-30T13:12:32.350546Z","shell.execute_reply.started":"2021-11-30T13:12:31.067004Z","shell.execute_reply":"2021-11-30T13:12:32.349662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Bar plots for categorical features","metadata":{}},{"cell_type":"code","source":"# no surprises here\nadultTest[\"workclass\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:32.35179Z","iopub.execute_input":"2021-11-30T13:12:32.352378Z","iopub.status.idle":"2021-11-30T13:12:32.615173Z","shell.execute_reply.started":"2021-11-30T13:12:32.352328Z","shell.execute_reply":"2021-11-30T13:12:32.614376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no surprises here\nadultTest[\"maritalStatus\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:32.616482Z","iopub.execute_input":"2021-11-30T13:12:32.616919Z","iopub.status.idle":"2021-11-30T13:12:32.888565Z","shell.execute_reply.started":"2021-11-30T13:12:32.616884Z","shell.execute_reply":"2021-11-30T13:12:32.887661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no surprises here\nadultTest[\"occupation\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:32.889625Z","iopub.execute_input":"2021-11-30T13:12:32.889823Z","iopub.status.idle":"2021-11-30T13:12:33.236517Z","shell.execute_reply.started":"2021-11-30T13:12:32.889798Z","shell.execute_reply":"2021-11-30T13:12:33.23562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no surprises here\nadultTest[\"race\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:33.237994Z","iopub.execute_input":"2021-11-30T13:12:33.238522Z","iopub.status.idle":"2021-11-30T13:12:33.620507Z","shell.execute_reply.started":"2021-11-30T13:12:33.238486Z","shell.execute_reply":"2021-11-30T13:12:33.619578Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# no surprises here\nadultTest[\"country\"].value_counts().plot(kind=\"bar\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:33.621714Z","iopub.execute_input":"2021-11-30T13:12:33.621945Z","iopub.status.idle":"2021-11-30T13:12:34.23144Z","shell.execute_reply.started":"2021-11-30T13:12:33.621919Z","shell.execute_reply":"2021-11-30T13:12:34.23069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Plots of target vs features","metadata":{}},{"cell_type":"markdown","source":"### Numerical features","metadata":{}},{"cell_type":"code","source":"## OBS: the plots below dont consider the dataset imbalaca, therefore, all ratios are essentially multiplied\n# by a factor of 2.3 in favour of <50K.","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:34.232997Z","iopub.execute_input":"2021-11-30T13:12:34.233212Z","iopub.status.idle":"2021-11-30T13:12:34.238148Z","shell.execute_reply.started":"2021-11-30T13:12:34.233186Z","shell.execute_reply":"2021-11-30T13:12:34.237266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for <30 it is almost certain that wage <50K; 30-40 roughly the same; 40-50 >50K has good advantage\n# <50K decays linearly with age, while 50K is like a normal function centered in 43\nsns.catplot(x=\"target\", y=\"age\", kind=\"violin\", inner=None, data=adultTrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:34.239355Z","iopub.execute_input":"2021-11-30T13:12:34.241533Z","iopub.status.idle":"2021-11-30T13:12:34.756919Z","shell.execute_reply.started":"2021-11-30T13:12:34.241486Z","shell.execute_reply":"2021-11-30T13:12:34.75601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for <10 <50K has a good advantage; 10-12.5 same; >12.5 >50K has very good advantage\nsns.catplot(x=\"target\", y=\"educationNum\", kind=\"violin\", inner=None, data=adultTrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:34.758405Z","iopub.execute_input":"2021-11-30T13:12:34.758684Z","iopub.status.idle":"2021-11-30T13:12:35.267483Z","shell.execute_reply.started":"2021-11-30T13:12:34.758652Z","shell.execute_reply":"2021-11-30T13:12:35.266465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for <40 <50K has a good advantage; for >40 >50K has a good advantage\nsns.catplot(x=\"target\", y=\"hoursPerWeek\", kind=\"violin\", inner=None, data=adultTrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:35.268798Z","iopub.execute_input":"2021-11-30T13:12:35.269062Z","iopub.status.idle":"2021-11-30T13:12:35.767736Z","shell.execute_reply.started":"2021-11-30T13:12:35.269022Z","shell.execute_reply":"2021-11-30T13:12:35.766673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"target\", y=\"capitalGain\", kind=\"violin\", inner=None, data=adultTrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:35.769508Z","iopub.execute_input":"2021-11-30T13:12:35.76982Z","iopub.status.idle":"2021-11-30T13:12:36.253748Z","shell.execute_reply.started":"2021-11-30T13:12:35.769776Z","shell.execute_reply":"2021-11-30T13:12:36.252833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.catplot(x=\"target\", y=\"capitalLoss\", kind=\"violin\", inner=None, data=adultTrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:36.259271Z","iopub.execute_input":"2021-11-30T13:12:36.259548Z","iopub.status.idle":"2021-11-30T13:12:36.751813Z","shell.execute_reply.started":"2021-11-30T13:12:36.25952Z","shell.execute_reply":"2021-11-30T13:12:36.750774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"code","source":"# OBS: I am multiplying the counts of >50K by the imbalaceRatio to decouple \n# the fact that the dataset is imbalaced from differences in distribution of the feature","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:36.753176Z","iopub.execute_input":"2021-11-30T13:12:36.753416Z","iopub.status.idle":"2021-11-30T13:12:36.757671Z","shell.execute_reply.started":"2021-11-30T13:12:36.753389Z","shell.execute_reply":"2021-11-30T13:12:36.756707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Private & Self-empinc differ a little, the rest is roughly the same, so can be grouoed \n# into a single category called other\ncountsDf = adultTrain[[\"target\",\"workclass\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:36.759098Z","iopub.execute_input":"2021-11-30T13:12:36.759744Z","iopub.status.idle":"2021-11-30T13:12:37.117563Z","shell.execute_reply.started":"2021-11-30T13:12:36.759693Z","shell.execute_reply":"2021-11-30T13:12:37.116494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all ctegories are different, thus maintaining all of them seems the way to go\ncountsDf = adultTrain[[\"target\",\"maritalStatus\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:37.118872Z","iopub.execute_input":"2021-11-30T13:12:37.119504Z","iopub.status.idle":"2021-11-30T13:12:37.434183Z","shell.execute_reply.started":"2021-11-30T13:12:37.119462Z","shell.execute_reply":"2021-11-30T13:12:37.433358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tranposrt moving, tech support, sales, creaf repair dont seem to help distringuish, so could be grouped\n# into a single category named rest\ncountsDf = adultTrain[[\"target\",\"occupation\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:37.435714Z","iopub.execute_input":"2021-11-30T13:12:37.436181Z","iopub.status.idle":"2021-11-30T13:12:37.866216Z","shell.execute_reply.started":"2021-11-30T13:12:37.436137Z","shell.execute_reply":"2021-11-30T13:12:37.865614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# black and other dimishes for over >50K but white dominates in both\n# I think grouping into white and non-white is a valid approach here\ncountsDf = adultTrain[[\"target\",\"race\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(10, 7))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:37.867164Z","iopub.execute_input":"2021-11-30T13:12:37.867713Z","iopub.status.idle":"2021-11-30T13:12:38.181778Z","shell.execute_reply.started":"2021-11-30T13:12:37.867652Z","shell.execute_reply":"2021-11-30T13:12:38.180833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# i think this can be mexico and non-mexico because the rest of the categories have so little data\n# that it is likely that we are fittng statistical noise\ncountsDf = adultTrain[[\"target\",\"country\"]].value_counts().unstack()\ncountsDf.loc[\">50K\", :] = countsDf.loc[\">50K\", :]*imbalanceRatio\ncountsDf.plot(kind=\"bar\", stacked=True,  figsize=(15, 10))","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:38.183017Z","iopub.execute_input":"2021-11-30T13:12:38.183444Z","iopub.status.idle":"2021-11-30T13:12:39.085721Z","shell.execute_reply.started":"2021-11-30T13:12:38.183408Z","shell.execute_reply":"2021-11-30T13:12:39.084451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pairwise plots","metadata":{}},{"cell_type":"markdown","source":"### Numerical vs numerical","metadata":{}},{"cell_type":"code","source":"# age limmits >50K even with high education and hours per week\n# age < 35 seems to be good indicator -> could maybe be binary variable\n\n# capitalGain > aprox 5 000 seems to be a great separator \n# capitalGain > 50 000 guarantees >50K \n# could be categorical variable\n\n# educationNum > 10 seems to be good indicator also\n \n# 1 000 < capital loss < 3 000 can be good \n\n# hours per week < 50 good\nsns.pairplot(adultTrain, hue=\"target\")","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:12:39.087314Z","iopub.execute_input":"2021-11-30T13:12:39.087678Z","iopub.status.idle":"2021-11-30T13:13:55.044007Z","shell.execute_reply.started":"2021-11-30T13:12:39.087648Z","shell.execute_reply":"2021-11-30T13:13:55.043225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Correlation plot","metadata":{}},{"cell_type":"code","source":"# all numerical features with very low correlation\nsns.heatmap(adultTrain.corr())","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:13:55.045427Z","iopub.execute_input":"2021-11-30T13:13:55.045885Z","iopub.status.idle":"2021-11-30T13:13:55.385372Z","shell.execute_reply.started":"2021-11-30T13:13:55.045836Z","shell.execute_reply":"2021-11-30T13:13:55.384325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Categorical heatmap","metadata":{}},{"cell_type":"code","source":"adultTrainDummies = pd.get_dummies(adultTrain[[\"workclass\", \"maritalStatus\", \"occupation\", \"race\", \"country\"]])\ndummy_features = adultTrainDummies.columns.values\npivots = []\nfor feature in dummy_features:\n    rest_of_features = dummy_features[dummy_features != feature]\n    new_pivot = adultTrainDummies.groupby(feature)[rest_of_features].sum().fillna(0)\n    pivots.append(new_pivot)\n\nfullPivot = pd.concat(pivots)[dummy_features]\nfullPivotOnes = fullPivot.iloc[lambda x: x.index > 0]\nfullPivotOnes.set_index(adultTrainDummies.columns, inplace=True)\n\ndef normalize_pivot_tables(fullPivot):\n    vec = np.array(fullPivot.sum(axis=1).values)\n    sizeDummies = vec.size\n    normMatrix = np.zeros((sizeDummies, sizeDummies))\n    for i, element in enumerate(vec):\n        for j, element2 in enumerate(vec):\n            normMatrix[i][j] = element + element2\n                        \n    normDf = pd.DataFrame(normMatrix, columns=fullPivot.columns)\n    normDf.set_index(fullPivot.columns, inplace=True)\n    fullPivotNorm = fullPivot.div(normDf)\n    return fullPivotNorm\n\nfullPivotNorm = normalize_pivot_tables(fullPivotOnes) # P(X1 = 1, X2 = 1)\n\n# dataset is too big, so will divide in two for plotting heatmaps\n#fullPivot2 = fullPivot.iloc[37:, :37] # down left -> not useful\n#fullPivot4 = fullPivot.iloc[37:, 37:] # down right -> country vs country -> not useful\nfullPivotNorm1 = fullPivotNorm.iloc[:37, :37] # top left\nfullPivotNorm3 = fullPivotNorm.iloc[:37:, 37:] # top right\n\n#OBS: dummy features with same prefix are mutually exclsusive, \n# therefore they will have joint prob equal to zero \n\n# max joint probability is aprox 0.1 in entire categorical combinations dataset, \n# therefore all categorical features are relatively independent from each other\n","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:13:55.386754Z","iopub.execute_input":"2021-11-30T13:13:55.38781Z","iopub.status.idle":"2021-11-30T13:13:57.838748Z","shell.execute_reply.started":"2021-11-30T13:13:55.387747Z","shell.execute_reply":"2021-11-30T13:13:57.837888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fullPivotNorm.describe()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:13:57.840253Z","iopub.execute_input":"2021-11-30T13:13:57.840618Z","iopub.status.idle":"2021-11-30T13:13:57.998774Z","shell.execute_reply.started":"2021-11-30T13:13:57.840573Z","shell.execute_reply":"2021-11-30T13:13:57.997807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm1,ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:13:58.000454Z","iopub.execute_input":"2021-11-30T13:13:58.000743Z","iopub.status.idle":"2021-11-30T13:13:59.276913Z","shell.execute_reply.started":"2021-11-30T13:13:58.000704Z","shell.execute_reply":"2021-11-30T13:13:59.276003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, ax = plt.subplots(figsize=(10,7))\nsns.heatmap(fullPivotNorm3, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:13:59.278546Z","iopub.execute_input":"2021-11-30T13:13:59.278855Z","iopub.status.idle":"2021-11-30T13:14:00.70732Z","shell.execute_reply.started":"2021-11-30T13:13:59.278815Z","shell.execute_reply":"2021-11-30T13:14:00.706397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data engineering\n### Prepare data for algorithm.","metadata":{}},{"cell_type":"markdown","source":"## Divide dataset into numerical and categorical subdatasets","metadata":{}},{"cell_type":"code","source":"numColumns = [\"age\", \"capitalGain\", \"capitalLoss\", \"educationNum\", \"hoursPerWeek\"] # obs: left weight out\ncatColumns = [\"country\", \"education\", \"maritalStatus\", \"occupation\", \"race\", \"relationship\", \"sex\", \"workclass\"] # obs: left target out\ntargetTrain = adultTrain[\"target\"]\nadultTrainNum = adultTrain[numColumns]\nadultTrainCat = adultTrain[catColumns]","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.708461Z","iopub.execute_input":"2021-11-30T13:14:00.708682Z","iopub.status.idle":"2021-11-30T13:14:00.716868Z","shell.execute_reply.started":"2021-11-30T13:14:00.708654Z","shell.execute_reply":"2021-11-30T13:14:00.716243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adultTrainNum.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.718028Z","iopub.execute_input":"2021-11-30T13:14:00.718502Z","iopub.status.idle":"2021-11-30T13:14:00.737526Z","shell.execute_reply.started":"2021-11-30T13:14:00.718458Z","shell.execute_reply":"2021-11-30T13:14:00.736821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"adultTrainCat.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.739387Z","iopub.execute_input":"2021-11-30T13:14:00.740062Z","iopub.status.idle":"2021-11-30T13:14:00.753771Z","shell.execute_reply.started":"2021-11-30T13:14:00.740019Z","shell.execute_reply":"2021-11-30T13:14:00.752947Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Normalize features","metadata":{}},{"cell_type":"code","source":"adultTrainNum = (adultTrainNum-adultTrainNum.mean())/adultTrainNum.std()\nadultTrainNum.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.754825Z","iopub.execute_input":"2021-11-30T13:14:00.755022Z","iopub.status.idle":"2021-11-30T13:14:00.770734Z","shell.execute_reply.started":"2021-11-30T13:14:00.754997Z","shell.execute_reply":"2021-11-30T13:14:00.770217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Treat categorical features","metadata":{}},{"cell_type":"code","source":"# Target-encoding \n# encoder = TargetEncoder()\n# encoder.fit_transform(adultTrainCat, adultTrain[\"target\"])\n# Simple one-hot encoding (this will be chosen one for now)\nadultTrainCat = pd.get_dummies(adultTrainCat)\nadultTrainCat.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.771769Z","iopub.execute_input":"2021-11-30T13:14:00.772123Z","iopub.status.idle":"2021-11-30T13:14:00.822938Z","shell.execute_reply.started":"2021-11-30T13:14:00.772093Z","shell.execute_reply":"2021-11-30T13:14:00.822104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Joining numerical and categorical dfs back","metadata":{}},{"cell_type":"code","source":"adultTrain = pd.concat([adultTrainNum, adultTrainCat], axis=1)\nadultTrain.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.824313Z","iopub.execute_input":"2021-11-30T13:14:00.824678Z","iopub.status.idle":"2021-11-30T13:14:00.84372Z","shell.execute_reply.started":"2021-11-30T13:14:00.824642Z","shell.execute_reply":"2021-11-30T13:14:00.842874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Treat missing values","metadata":{}},{"cell_type":"code","source":"# Two main options\n# 1. Just thorw away rows with missing values\n# 2. Replace with mean of colummn (this will be chosen one for now)\nadultTrain.fillna(adultTrain.mean(), inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.845137Z","iopub.execute_input":"2021-11-30T13:14:00.845459Z","iopub.status.idle":"2021-11-30T13:14:00.88045Z","shell.execute_reply.started":"2021-11-30T13:14:00.84542Z","shell.execute_reply":"2021-11-30T13:14:00.879659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Treat outliers","metadata":{}},{"cell_type":"code","source":"# todo later","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.881763Z","iopub.execute_input":"2021-11-30T13:14:00.882282Z","iopub.status.idle":"2021-11-30T13:14:00.885993Z","shell.execute_reply.started":"2021-11-30T13:14:00.88225Z","shell.execute_reply":"2021-11-30T13:14:00.885072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Feature tranformations","metadata":{}},{"cell_type":"code","source":"# todo later","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.887118Z","iopub.execute_input":"2021-11-30T13:14:00.887821Z","iopub.status.idle":"2021-11-30T13:14:00.897185Z","shell.execute_reply.started":"2021-11-30T13:14:00.887786Z","shell.execute_reply":"2021-11-30T13:14:00.896383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Mirror on test dataset","metadata":{}},{"cell_type":"code","source":"adultTestNum = adultTest[numColumns]\nadultTestCat = adultTest[catColumns]\n\nadultTestNum = (adultTestNum-adultTestNum.mean())/adultTestNum.std() # broadcasts to columns by default\n\nadultTestCat = pd.get_dummies(adultTestCat)\nadultTestCat = adultTestCat.reindex(columns = adultTrainCat.columns, fill_value=0) # equivalent to fit transform\n\nadultTest = pd.concat([adultTestNum, adultTestCat], axis=1)\n\nadultTest.fillna(adultTest.mean(), inplace=True)\n\nadultTest.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.900423Z","iopub.execute_input":"2021-11-30T13:14:00.900785Z","iopub.status.idle":"2021-11-30T13:14:00.971051Z","shell.execute_reply.started":"2021-11-30T13:14:00.900748Z","shell.execute_reply":"2021-11-30T13:14:00.970391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Engineering\n### Select and/or create new features. Non-linear transformations affect more KNN performance","metadata":{}},{"cell_type":"code","source":"# Primising Engineered Datasets (v0)\n# Disct which will hold different feature engineered candidate datsets\npromisingDatasets = {}","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.972207Z","iopub.execute_input":"2021-11-30T13:14:00.972608Z","iopub.status.idle":"2021-11-30T13:14:00.97592Z","shell.execute_reply.started":"2021-11-30T13:14:00.972567Z","shell.execute_reply":"2021-11-30T13:14:00.975053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importance sampling","metadata":{}},{"cell_type":"code","source":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # with the given weights, the rows can be resampled according to their weight\n# # number of rows = weight_factor*minMaxNormalized_weight where the weight factor is large enough so that the sampling\n# # can give different integer values for most of the rows, \n# # but if it is too large becomes computationally heavy\n# weightTrainNorm = ((weightTrain) - weightTrain.min())/(weightTrain.max() - weightTrain.min())\n# weightFactor = 50 # (can be altered later)\n# knnNeighboursFactor = weightTrainNorm.mean()*50 # expected value of number of columns added\n# print('knnNeighboursFactor:', knnNeighboursFactor)\n\n# # adultTrain70Importance will contain replicas only of row in it, will used to train KNN, \n# # that will be tested in adultTrain30ImportanceCV, which wasnt replicated\n# adultTrainShuffled = adultTrain.sample(frac=1)\n# adultTrain70Importance, adultTrain30Importance = \\\n#     np.split(adultTrainShuffled, [int(.7*len(adultTrain))])\n# adultTrain30ImportanceCopy = adultTrain30Importance.copy()\n# # putting back target I removed earlier\n# adultTrain70Importance[\"target\"] = targetTrain[:len(adultTrain70Importance)]\n\n# for idx, row in adultTrain70Importance.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain70Importance = adultTrain70Importance.append([df]*numReplicatedRows, ignore_index=True)\n    \n# for idx, row in adultTrain30ImportanceCopy.iterrows():\n#     numReplicatedRows = int(weightTrainNorm[idx]*weightFactor)\n#     df = row.to_frame().T\n#     adultTrain30ImportanceCopy = adultTrain30ImportanceCopy.append([df]*numReplicatedRows, ignore_index=True)\n\n# #### IMPORTANT: this is 100% of the actual training dataset -> only used if survived CV\n# adultTrainImportance = pd.concat([adultTrain70Importance, adultTrain30ImportanceCopy])\n# promisingDatasets[\"importanceSampling\"] = adultTrainImportance","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.977227Z","iopub.execute_input":"2021-11-30T13:14:00.977509Z","iopub.status.idle":"2021-11-30T13:14:00.991905Z","shell.execute_reply.started":"2021-11-30T13:14:00.977476Z","shell.execute_reply":"2021-11-30T13:14:00.990977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Select features","metadata":{}},{"cell_type":"code","source":"# todo later","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:00.993254Z","iopub.execute_input":"2021-11-30T13:14:00.994152Z","iopub.status.idle":"2021-11-30T13:14:01.007075Z","shell.execute_reply.started":"2021-11-30T13:14:00.994112Z","shell.execute_reply":"2021-11-30T13:14:01.006433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create new features","metadata":{}},{"cell_type":"code","source":"# todo later","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:01.008106Z","iopub.execute_input":"2021-11-30T13:14:01.009477Z","iopub.status.idle":"2021-11-30T13:14:01.018497Z","shell.execute_reply.started":"2021-11-30T13:14:01.009418Z","shell.execute_reply":"2021-11-30T13:14:01.017815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments\n### Tune and compare 4 different classifiers. These are: Decision Trees (RF and XGBoost), SVM and NN.","metadata":{}},{"cell_type":"code","source":"# Label encoder\nle = preprocessing.LabelEncoder()\n# Test data\nXtest = adultTest.values\n\nSEED = 10","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:01.019631Z","iopub.execute_input":"2021-11-30T13:14:01.019991Z","iopub.status.idle":"2021-11-30T13:14:01.033869Z","shell.execute_reply.started":"2021-11-30T13:14:01.019961Z","shell.execute_reply":"2021-11-30T13:14:01.032723Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Base dataset","metadata":{}},{"cell_type":"code","source":"#### Baseline dataset\nXtrain = adultTrain.values\nYtrain = le.fit_transform(targetTrain)\n\n# shape check\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(Ytrain.shape)","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:01.035152Z","iopub.execute_input":"2021-11-30T13:14:01.035415Z","iopub.status.idle":"2021-11-30T13:14:01.059531Z","shell.execute_reply.started":"2021-11-30T13:14:01.035381Z","shell.execute_reply":"2021-11-30T13:14:01.058957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Baseline (KNN)","metadata":{}},{"cell_type":"code","source":"baselineKnn = KNeighborsClassifier()\nbaselineKnnAcc = cross_val_score(baselineKnn, Xtrain, Ytrain, cv=5, scoring='accuracy')\nbaselineKnnAccMean = baselineKnnAcc.mean()\nprint('mean accuracy for baseline knn: ', baselineKnnAccMean)\n\nbaselineKnn.fit(Xtrain, Ytrain)\n\ncurrentBestModel = {\n    'model': baselineKnn,\n    'modelFamily': 'KNN',\n    'cv': baselineKnnAccMean, \n    'X': Xtrain,\n    'Y': Ytrain\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:14:01.060604Z","iopub.execute_input":"2021-11-30T13:14:01.060949Z","iopub.status.idle":"2021-11-30T13:16:00.249938Z","shell.execute_reply.started":"2021-11-30T13:14:01.060913Z","shell.execute_reply":"2021-11-30T13:16:00.249049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4 Classifiers","metadata":{}},{"cell_type":"markdown","source":"### RF","metadata":{}},{"cell_type":"code","source":"rf = RandomForestClassifier(random_state=SEED)\n\nrfConfig = {\n    'n_estimators': np.arange(10, 50),\n    'criterion': ['gini', 'entropy'],\n    'max_depth': np.arange(5, 50)\n}\n\nrfRandomSearch = (\n    RandomizedSearchCV(\n        rf, \n        rfConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nrfRandomSearch.fit(Xtrain, Ytrain)\n\nrfMean = rfRandomSearch.best_score_\nprint('mean accuracy for rf: ', rfMean)\n\nrfTuned = rfRandomSearch.best_estimator_\nprint('tuned RF: ', rfTuned)\n\nif rfMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': rfTuned,\n        'modelFamily': 'RF',\n        'cv': rfMean\n    }","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:16:00.251091Z","iopub.execute_input":"2021-11-30T13:16:00.251868Z","iopub.status.idle":"2021-11-30T13:16:09.868069Z","shell.execute_reply.started":"2021-11-30T13:16:00.251824Z","shell.execute_reply":"2021-11-30T13:16:09.867347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"code","source":"xgb = XGBClassifier(random_state=SEED, use_label_encoder=False)\n\nxgbConfig = {\n    'n_estimators': np.arange(10, 50),\n    'learning_rate': np.arange(1e-3, 1),\n    'max_depth': np.arange(5, 50),\n    'reg_alpha': [1e-5, 1e-2, 0.1, 1, 100],\n    'reg_lambda': [1e-5, 1e-2, 0.1, 1, 100]\n}\n\nxgbRandomSearch = (\n    RandomizedSearchCV(\n        xgb, \n        xgbConfig, \n        verbose=False, \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nxgbRandomSearch.fit(Xtrain, Ytrain)\n\nxgbMean = xgbRandomSearch.best_score_\nprint('mean accuracy for xgb: ', xgbMean)\n\nxgbTuned = xgbRandomSearch.best_estimator_\nprint('tuned XGB: ', xgbTuned)\n\nif xgbMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': xgbTuned,\n        'modelFamily': 'XGB',\n        'cv': xgbMean\n    }","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:16:09.869453Z","iopub.execute_input":"2021-11-30T13:16:09.870067Z","iopub.status.idle":"2021-11-30T13:22:25.994133Z","shell.execute_reply.started":"2021-11-30T13:16:09.870028Z","shell.execute_reply":"2021-11-30T13:22:25.993265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### SVM","metadata":{}},{"cell_type":"code","source":"svm = SVC(random_state=SEED, probability=True)\n\nsvmConfig = {\n    'C': np.arange(1e-3, 10),\n    'gamma': ['scale', 'auto']\n}\n\nsvmRandomSearch = (\n    RandomizedSearchCV(\n        svm, \n        svmConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, # all cores\n        random_state=SEED\n    )\n)\n\nsvmRandomSearch.fit(Xtrain, Ytrain)\n\nsvmMean = svmRandomSearch.best_score_\nprint('mean accuracy for svm: ', svmMean)\n\nsvmTuned = svmRandomSearch.best_estimator_\nprint('tuned SVM: ', svmTuned)\n\nif svmMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': svmTuned,\n        'modelFamily': 'SVM',\n        'cv': svmMean\n    }","metadata":{"execution":{"iopub.status.busy":"2021-11-30T13:22:25.995728Z","iopub.execute_input":"2021-11-30T13:22:25.996013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### NN","metadata":{}},{"cell_type":"code","source":"# model definition\nnn = MLPClassifier(random_state=SEED, early_stopping=True)\n\n# RandomizedSearchCV\nnnConfig = {\n    'hidden_layer_sizes': [(2 ** i,) for i in np.arange(2, 7)], # just one hidden layer\n    'alpha': [1e-10, 1e-8, 1e-6, 1e-4, 1e-2, 1e-0, 1e2],\n    'learning_rate': ['constant', 'adaptive']\n}\n\nnnRandomSearch = (\n    RandomizedSearchCV(\n        nn, \n        nnConfig, \n        verbose=False, \n        scoring='accuracy', \n        cv=5, \n        n_iter=3, \n        n_jobs=-1, \n        random_state=SEED\n    )\n)\n\nnnRandomSearch.fit(Xtrain, Ytrain)\n\nnnMean = nnRandomSearch.best_score_\nprint('mean accuracy for svm: ', nnMean)\n\nnnTuned = nnRandomSearch.best_estimator_\nprint('tuned NN: ', nnTuned)\n\nif nnMean > currentBestModel['cv']:\n    currentBestModel = {\n        'model': nnTuned,\n        'modelFamily': 'NN',\n        'cv': nnMean\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Engineered datasets","metadata":{}},{"cell_type":"code","source":"############### NOTEBOOK WAS TO SLOW WITH THIS, DECIDED TO COMMENT OUT ###################\n\n# # 1. Importance Sampling dataset\n# Ytrain70ImportanceNotEncoded = adultTrain70Importance.pop(\"target\").values\n# print('Ytrain70ImportanceNotEncoded:', Ytrain70ImportanceNotEncoded) \n# Ytrain70Importance = le.fit_transform(Ytrain70ImportanceNotEncoded)\n# print('Ytrain70Importance:', Ytrain70Importance) ## remove afterwards\n# Xtrain70Importance = adultTrain70Importance.values\n\n# # for cross validation \n# # k-fold cross validation is not done here because, the duplicated rows would leak to the cv sets\n# #Ytrain30ImportanceNotEncoded = adultTrain30Importance.pop(\"target\")\n# Ytrain30Importance = le.fit_transform(Ytrain30ImportanceNotEncoded)\n# Xtrain30Importance = adultTrain30Importance.values\n\n# # shape check\n# print(Xtrain70Importance.shape)\n# print(Xtest.shape) \n# print(Ytrain70Importance.shape)\n\n# # 5 is the deafult n_neighbors\n# importanceSamplingKnn = KNeighborsClassifier(n_neighbors=int(5*knnNeighboursFactor))\n# importanceSamplingKnn.fit(Xtrain70Importance, Ytrain70Importance)\n# Ytrain30Prediction = importanceSamplingKnn.predict(Xtrain30Importance)\n# print('Ytrain30Prediction', Ytrain30Prediction) ### remove afterwards\n# importanceSamplingKnnAccMean = accuracy_score(Ytrain30Importance, Ytrain30Prediction)\n\n# print('mean accuracy for importanceSamplingKnnAccMean: ', importanceSamplingKnnAccMean)\n# if importanceSamplingKnnAccMean > currentBestModel['cv']:  \n#     # get whole dataset fro promisingDatasets\n#     adultTrainImportance = promisingDatasets[\"importanceSampling\"] \n    \n#     YtrainImportance = le.fit_transform(adultTrainImportance.pop(\"target\").values)\n#     XtrainImportance = adultTrainImportance.values\n        \n#     currentBestModel = {\n#         'model': importanceSamplingKnn,\n#         'cv': importanceSamplingKnnAccMean,\n#         'X': XtrainImportance,\n#         'Y': YtrainImportance\n#     }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Final model\n### Trained on entire train dataset.","metadata":{}},{"cell_type":"code","source":"bestFamily = currentBestModel['modelFamily']\ntrainedBestModel = currentBestModel['model']\nmeanAcc = currentBestModel['cv']\n\nprint('best family: ', bestFamily)\nprint('best model: ', trainedBestModel)\nprint('best cv mean accuracy: ', meanAcc)\n\npredictions = trainedBestModel.predict(Xtest) # numpy array","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission\n### Save to csv in the required format.","metadata":{}},{"cell_type":"code","source":"# going back to array of strings <=50 K and >50K\npredictions = le.inverse_transform(predictions)\nsubmissionDf = pd.DataFrame({'Id': idTest.values, 'income': predictions})\n\nsubmissionDf.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}