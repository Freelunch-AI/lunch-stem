{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "tut04-f20.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOaagFfQIT16"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "print(torch.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tYICxJUrUjs"
      },
      "source": [
        "!pip install ipdb\n",
        "import ipdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDGKdlknJMuK"
      },
      "source": [
        "# PyTorch: The Basics\n",
        "\n",
        "PyTorch allows you to dynamically define computational graphs. This is done by operating on `Variable`s, which wrap PyTorch's `Tensor` objects.\n",
        "\n",
        "Here is an example, where we work with the function\n",
        "\n",
        "$$f(x) = x^2 + 2x + 6$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFyOEjoMIdys"
      },
      "source": [
        "def f(x):\n",
        "    return x ** 2 + 2 * x + 6\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65doQHZOIgLL"
      },
      "source": [
        "np_x = np.array([4.0])\n",
        "x = torch.from_numpy(np_x).requires_grad_(True)\n",
        "y = f(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUdJKPjoIzkN"
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qCWi1fPI1Kj"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZtfuqTgI22f"
      },
      "source": [
        "np_x = np.array([5.0])\n",
        "x = torch.from_numpy(np_x).requires_grad_(True)\n",
        "y = f(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ML7UhKGEI-RX"
      },
      "source": [
        "y.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYaNJ69eJAlm"
      },
      "source": [
        "x.grad"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1H4XJ-uJovy"
      },
      "source": [
        "Unlike Tensorflow, we can define the graph on the fly. That is why it is more convenient to define a function in Python: we call the function as part of constructing the graph.\n",
        "\n",
        "# Linear Classification with Pytorch\n",
        "\n",
        "Let's now create a simple linear function for classifiying MNIST digits. Material is lifted from: https://github.com/fastai/fastai_old/blob/master/dev_nb/001a_nn_basics.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4lMBUMSYaNy"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWHpTqTIJBZ0"
      },
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "mnist_train = datasets.MNIST('data', train=True, download=True,\n",
        "                       transform=transforms.ToTensor())\n",
        "\n",
        "mnist_test = datasets.MNIST('../data', train=False, download=True, transform=\n",
        "                            transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_XFXXh4WXBl"
      },
      "source": [
        "print(mnist_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7wt5b3PX2YW"
      },
      "source": [
        "i = 732  # try different indices\n",
        "example = mnist_train[i]\n",
        "print(\"Label: \", example[1])\n",
        "plt.imshow(example[0].reshape((28,28)), cmap = plt.cm.gray)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVqIkU_jxF8X"
      },
      "source": [
        "Pytorch's DataLoader is responsible for managing batches. You can create a DataLoader from any Dataset. DataLoader makes it easier to iterate over batches (it can shuffle and give you the next batch)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN4zzJnmY-XD"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_dl = DataLoader(mnist_train, batch_size=100, shuffle=False)\n",
        "\n",
        "dataiter = iter(train_dl)\n",
        "images, labels = dataiter.next()\n",
        "viz = torchvision.utils.make_grid(images, nrow=10, padding = 2).numpy()\n",
        "fig, ax = plt.subplots(figsize= (8,8))\n",
        "ax.imshow(np.transpose(viz, (1,2,0)))\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsJe7q95aR0X"
      },
      "source": [
        "Thanks to PyTorch's ability to calculate gradients automatically, we can use any standard Python function (or callable object) as a model! So let's just write a plain matrix multiplication and broadcasted addition to create a simple linear model. We also need an activation function, so we'll write log_softmax and use it. Remember: although PyTorch provides lots of pre-written loss functions, activation functions, and so forth, you can easily write your own using plain python. PyTorch will even create fast GPU or vectorized CPU code for your function automatically.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5awYPvQpaTuw"
      },
      "source": [
        "def log_softmax(x): \n",
        "    return x - x.exp().sum(-1).log().unsqueeze(-1)\n",
        "\n",
        "def model(xb, weights, bias):      \n",
        "    return log_softmax(xb @ weights + bias)\n",
        "\n",
        "def nll(input, target): \n",
        "    # input (prediction) shape = (batch_size, 10)\n",
        "    # target shape = (batch_sise,)\n",
        "    # input[range(batch_size), target] shape = (batch_size,)\n",
        "    return -input[range(target.shape[0]), target].mean()\n",
        "\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds==yb).float().mean()\n",
        "\n",
        "loss_func = nll"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAPJAJN6ahSb"
      },
      "source": [
        "In the above, the '@' is syntactic sugar for the matrix multiply operation. We will call our function on one batch of data (in this case, 64 images). This is one forward pass. Note that our predictions won't be any better than random at this stage, since we start with random weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kx8A9ZmLagtW"
      },
      "source": [
        "lr = 0.1\n",
        "epochs = 1\n",
        "bs = 128\n",
        "\n",
        "print_every = 100\n",
        "in_shape = 784\n",
        "out_shape = 10\n",
        "\n",
        "train_dl = DataLoader(mnist_train, batch_size=bs)\n",
        "test_dl = DataLoader(mnist_test, batch_size = 100)\n",
        "\n",
        "# Initialize weights\n",
        "weights = torch.randn(in_shape, out_shape) / math.sqrt(in_shape)\n",
        "weights.requires_grad_()\n",
        "bias = torch.zeros(out_shape, requires_grad=True)\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for i, (xb, yb) in enumerate(train_dl):\n",
        "    \n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        \n",
        "        # Evaluate training accuracy\n",
        "        if i % print_every == 0: \n",
        "            print(\"Batch: \", i)\n",
        "            print(\"Train acc on curr batch: \", accuracy(model(xb, weights, bias), yb).item())\n",
        "            \n",
        "        # Forward pass    \n",
        "        pred = model(xb, weights, bias)\n",
        "        loss = loss_func(pred, yb)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        with torch.no_grad():   # temporarily sets all the requires_grad flags to False\n",
        "            weights -= weights.grad * lr\n",
        "            bias -= bias.grad * lr\n",
        "            weights.grad.zero_()\n",
        "            bias.grad.zero_()\n",
        "            \n",
        "        # Evaluate training accuracy\n",
        "        if i % print_every == 0: \n",
        "            print(\"Train acc on curr batch (post-update): \", accuracy(model(xb, weights, bias), yb).item())\n",
        "     \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPI_vEbt0qQY"
      },
      "source": [
        "Because the classifer is a linear model, the weight parameters connected to each output class can be viewed as an image as well (28x28, same size as the input). We visualize the weights below. They tell us how the linear classifer weighs the pixels of the input image to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLHUDjhXy85h"
      },
      "source": [
        "nrows = 2\n",
        "ncols = 5\n",
        "weights_np = weights.detach().cpu().numpy()\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, )\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        # axes[i, j].imshow(np.maximum(0.0, weights_np[:, i * ncols + j]).reshape((28, 28)), cmap='gray')#plt.cm.coolwarm)\n",
        "        axes[i, j].imshow(weights_np[:, i * ncols + j].reshape((28, 28)), cmap='gray')#plt.cm.coolwarm)\n",
        "        axes[i, j].set_xticks([])\n",
        "        axes[i, j].set_yticks([])\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndFiHygPuhy9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_H6uUr9Unkdi"
      },
      "source": [
        "The above training loop is a bit clunky and error-prone. Moreover, the code would get very messy if our model is larger and more complicated. In the following example, we will take advantage of Pytorch's built-in functionalities to build more powerful models.\n",
        "\n",
        "# Neural Network with Pytorch\n",
        "\n",
        " We first introduce a helper function for evaluating neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnwZNcARdVz9"
      },
      "source": [
        "def get_test_stat(model, dl, device):\n",
        "    model.eval()    # set model to eval mode\n",
        "    cum_loss, cum_acc = 0.0, 0.0\n",
        "    for i, (xb, yb) in enumerate(dl):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        \n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        y_pred = model(xb)\n",
        "        loss = loss_fn(y_pred, yb)\n",
        "        acc = accuracy(y_pred, yb)\n",
        "        cum_loss += loss.item() * len(yb)\n",
        "        cum_acc += acc.item() * len(yb)\n",
        "    cum_loss /= 10000\n",
        "    cum_acc /= 10000\n",
        "    model.train()   # set model back to train mode\n",
        "    return cum_loss, cum_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zg-kIf6oijc_"
      },
      "source": [
        "Then, we build a neural network with one hidden layer, by extending the `torch.nn.Module` class. This allows us to keep the code modularized, and is how larger and more complicated models (e.g. ConvNets) are usually built in Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8cvw7UThl4S"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        # define the parameters here\n",
        "        self.fc = torch.nn.Linear(input_dim, hidden_dim)          # first layer\n",
        "        self.out_layer = torch.nn.Linear(hidden_dim, output_dim)  # output layer\n",
        "    \n",
        "    def forward(self, x):   # defines the forward pass (overwriting the default method)\n",
        "        out = self.fc(x)            # pass input through the first layer\n",
        "        out = F.relu(out)           # apply ReLU activation\n",
        "        out = self.out_layer(out)   # pass through the output layer\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7CKHUqwlCmd"
      },
      "source": [
        "We can now train the network. Note that instead of manually updating the weights ourselves, we use a built-in Pytorch optimizer here `torch.optim.SGD`. Many other optimizers are available too (https://pytorch.org/docs/stable/optim.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2ItDIThddhe"
      },
      "source": [
        "learning_rate = 1e-2\n",
        "epochs = 10\n",
        "\n",
        "dim_x = 784\n",
        "dim_h = 100\n",
        "dim_out = 10\n",
        "\n",
        "# instantiate the model\n",
        "model = Net(dim_x, dim_h, dim_out)\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# create datasets and data loader\n",
        "mnist_train = datasets.MNIST('data', train=True, download=True,\n",
        "                       transform=transforms.ToTensor())\n",
        "\n",
        "mnist_test = datasets.MNIST('../data', train=False, download=True, transform=\n",
        "                            transforms.ToTensor())\n",
        "train_dl = DataLoader(mnist_train, batch_size=bs)\n",
        "test_dl = DataLoader(mnist_test, batch_size = 100)\n",
        "\n",
        "# Using GPUs in PyTorch is pretty straightforward\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using cuda\")\n",
        "    use_cuda = True\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "model.to(device)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# set the model to training mode\n",
        "model.train()\n",
        "\n",
        "train_stats = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'acc': []\n",
        "}\n",
        "test_stats = {\n",
        "    'epoch': [],\n",
        "    'loss': [],\n",
        "    'acc': []\n",
        "}\n",
        "\n",
        "pbar = tqdm(range(epochs))\n",
        "for epoch in pbar:\n",
        "    pbar.set_description(f\"Epoch {epoch + 1} / 10\")\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    for i, (xb, yb) in enumerate(train_dl):\n",
        "        xb = xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        \n",
        "        # Forward pass\n",
        "        y_pred = model(xb)\n",
        "        loss = loss_fn(y_pred, yb)\n",
        "        acc = accuracy(y_pred, yb)\n",
        "        # Backward pass\n",
        "        model.zero_grad()  # Zero out the previous gradient computation\n",
        "        loss.backward()    # Compute the gradient\n",
        "        optimizer.step()   # Use the gradient information to make a step\n",
        "        train_stats['epoch'].append(epoch + i / len(train_dl))\n",
        "        train_stats['loss'].append(loss.item())\n",
        "        train_stats['acc'].append(acc.item())\n",
        "    \n",
        "    test_loss, test_acc = get_test_stat(model, test_dl, device)\n",
        "    test_stats['epoch'].append(epoch + 1)\n",
        "    test_stats['loss'].append(test_loss)\n",
        "    test_stats['acc'].append(test_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoEzaOypwZLG"
      },
      "source": [
        "Plot training and test loss & accuracy curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljLJGE8_t3zd"
      },
      "source": [
        "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(6, 6))\n",
        "axes[0].plot(train_stats['epoch'], train_stats['loss'], label='train')\n",
        "axes[0].plot(test_stats['epoch'], test_stats['loss'], label='test')\n",
        "axes[0].set_xlabel('Epoch')\n",
        "axes[0].set_ylabel('Loss')\n",
        "axes[1].plot(train_stats['epoch'], train_stats['acc'], label='train')\n",
        "axes[1].plot(test_stats['epoch'], test_stats['acc'], label='test')\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBddFV1gzzxO"
      },
      "source": [
        "\n",
        "## Weight visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0Ysejq397m"
      },
      "source": [
        "We visualize the learned weights in the first layer of the network as images. Compared to the linear model before, this model has 100 hidden units with ReLU activation, enabling it to make use of a more diverse set of features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLEatAmV27uz"
      },
      "source": [
        "nrows = 10\n",
        "ncols = 10\n",
        "first_layer_weights = model.fc.weight.detach().cpu().numpy()\n",
        "fig, axes = plt.subplots(nrows=ncols, ncols=ncols, figsize=(6, 6))\n",
        "for i in range(nrows):\n",
        "    for j in range(ncols):\n",
        "        axes[i, j].imshow(first_layer_weights[i * ncols + j].reshape((28, 28)), cmap='gray')\n",
        "        axes[i, j].set_xticks([])\n",
        "        axes[i, j].set_yticks([])\n",
        "plt.tight_layout(pad=0.1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPKfs40iFPWX"
      },
      "source": [
        "## [Exercise] What input activates an output class the most?\n",
        "Unlike the linear model, we cannot directly visualize the weights connected to each output class. Instead, we will solve an optimization problem. Specifically, we will:\n",
        "\n",
        "1. Choose an output class you'd like to visualize (e.g. '2'). Initialize the input to be a random image.\n",
        "2. Compute the forward pass through the network.\n",
        "3. Compute the gradient of the output unit w.r.t. the *input*.\n",
        "4. Do a gradient ascent step on the *input*.\n",
        "5. Repeat step 2-4 until a satisfactory visualization is obtained.\n",
        "\n",
        "Optimizing the input to maximize the activation of some feature unit (in this case, an output unit) is a technique for feature visualization. This exercise only gives a naive example, and would probably not produce satisfactory visualizations on larger models and more diverse datasets (e.g. large ConvNet models on ImageNet). See [this article](https://distill.pub/2017/feature-visualization/) for techniques and fun examples of feature visualizations on GoogleNet trained on Imagenet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQTnjBY0KNu-"
      },
      "source": [
        "input_lr = 0.01\n",
        "input_train_itr = 200\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(6, 3))\n",
        "pbar = tqdm(range(10))\n",
        "\n",
        "\n",
        "for class_i in pbar:\n",
        "    pbar.set_description(f\"Visualizing class '{class_i}'\")\n",
        "\n",
        "    # 1. Initialize the input image.\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    # we need to do optimization on imput_img, hence set the requires_grad flag\n",
        "    input_img.requires_grad = True\n",
        "\n",
        "    # define the gradient descent optimizer on input_img\n",
        "    input_optim = torch.optim.SGD([input_img], lr=input_lr)\n",
        "\n",
        "    for train_i in range(input_train_itr):\n",
        "        # 2. Compute the forward pass through the network.\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        # 3. backward pass\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        # 4. Do a gradient ascent step on the input\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "    # plot the optimized input_img\n",
        "    axes[class_i // 5, class_i % 5].imshow(input_img.detach().cpu().numpy().reshape((28, 28)), cmap='gray')\n",
        "    axes[class_i // 5, class_i % 5].set_xticks([])\n",
        "    axes[class_i // 5, class_i % 5].set_yticks([])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}